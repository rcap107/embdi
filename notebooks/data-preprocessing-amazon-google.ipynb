{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Amazon-Google datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/spoutnik23/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import re\n",
    "import random as rd\n",
    "import datetime\n",
    "from datasketch import MinHash, MinHashLSH, MinHashLSHForest\n",
    "from similarity.levenshtein import Levenshtein\n",
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import EmbDI.data_preprocessing as dp\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1363, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>clickart 950 000 premier image pack ( dvd-rom )</td>\n",
       "      <td>broderbund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ca international arcserve lap/desktop oem 30pk</td>\n",
       "      <td>computer associates</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>noah 's ark activity center ( jewel case ages ...</td>\n",
       "      <td>victory multimedia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>peachtree by sage premium accounting for nonpr...</td>\n",
       "      <td>sage software</td>\n",
       "      <td>599.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>singing coach unlimited</td>\n",
       "      <td>carry-a-tune technologies</td>\n",
       "      <td>99.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0    clickart 950 000 premier image pack ( dvd-rom )   \n",
       "1   1     ca international arcserve lap/desktop oem 30pk   \n",
       "2   2  noah 's ark activity center ( jewel case ages ...   \n",
       "3   3  peachtree by sage premium accounting for nonpr...   \n",
       "4   4                            singing coach unlimited   \n",
       "\n",
       "                manufacturer   price  \n",
       "0                 broderbund     NaN  \n",
       "1        computer associates     NaN  \n",
       "2         victory multimedia     NaN  \n",
       "3              sage software  599.99  \n",
       "4  carry-a-tune technologies   99.99  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = '../pipeline/experiments/amazon-google/exp_data/tableA.csv'\n",
    "df1 = pd.read_csv(f1, encoding='utf-8')\n",
    "print(df1.shape)\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3226, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>learning quickbooks 2007</td>\n",
       "      <td>intuit</td>\n",
       "      <td>38.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>superstart ! fun with reading &amp; writing !</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>qb pos 6.0 basic software</td>\n",
       "      <td>intuit</td>\n",
       "      <td>637.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>math missions : the amazing arcade adventure (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>production prem cs3 mac upgrad</td>\n",
       "      <td>adobe software</td>\n",
       "      <td>805.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title    manufacturer  \\\n",
       "0   0                           learning quickbooks 2007          intuit   \n",
       "1   1          superstart ! fun with reading & writing !             NaN   \n",
       "2   2                          qb pos 6.0 basic software          intuit   \n",
       "3   3  math missions : the amazing arcade adventure (...             NaN   \n",
       "4   4                     production prem cs3 mac upgrad  adobe software   \n",
       "\n",
       "    price  \n",
       "0   38.99  \n",
       "1    8.49  \n",
       "2  637.99  \n",
       "3   12.95  \n",
       "4  805.99  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = '../pipeline/experiments/amazon-google/exp_data/tableB.csv'\n",
    "df2 = pd.read_csv(f2, encoding='utf-8')\n",
    "print(df2.shape)\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the Data Preprocessing parameters for the basic case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'output_file': 'amazon-google',\n",
    "    'concatenate': 'outer',\n",
    "    'missing_value': 'nan,ukn,none,unknown,',\n",
    "    'missing_value_strategy': '',\n",
    "    'round_number': 0,\n",
    "    'round_columns': 'price',\n",
    "    'auto_merge': False,\n",
    "    'expand_columns': ','.join(list(set(df1.columns))),\n",
    "    'tokenize_shared': False \n",
    "}\n",
    "\n",
    "df_c = dp.data_preprocessing([df1, df2], parameters)\n",
    "df_c = df_c.drop('id', axis=1)\n",
    "\n",
    "df_c.to_csv('../pipeline/datasets/amazon_google/amazon_google-master.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.write_info_file([df1, df2], 'info-amazon_google', [f1,f2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the schema matching dataset in the basic case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'output_file': 'amazon-google',\n",
    "    'concatenate': 'horizon',\n",
    "    'missing_value': 'nan,ukn,none,unknown,',\n",
    "    'missing_value_strategy': '',\n",
    "    'round_number': 0,\n",
    "    'round_columns': 'price',\n",
    "    'auto_merge': False,\n",
    "    'expand_columns': ','.join(list(set(df1.columns))),\n",
    "    'tokenize_shared': False \n",
    "}\n",
    "\n",
    "df_c = dp.data_preprocessing([df1.drop('id', axis=1), df2.drop('id', axis=1)], parameters)\n",
    "\n",
    "df_c.to_csv('../pipeline/datasets/amazon_google/amazon_google-master-sm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.write_info_file([df1, df2], 'info-amazon_google', [f1,f2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the Data Preprocessing parameters for the heuristic case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'output_file': 'amazon-google',\n",
    "    'concatenate': 'outer',\n",
    "    'missing_value': 'nan,ukn,none,unknown,',\n",
    "    'missing_value_strategy': '',\n",
    "    'round_number': 0,\n",
    "    'round_columns': 'price',\n",
    "    'auto_merge': False,\n",
    "    'tokenize_shared': True \n",
    "}\n",
    "\n",
    "df_c = dp.data_preprocessing([df1, df2], parameters)\n",
    "df_c = df_c.drop('id', axis=1)\n",
    "df_c.to_csv('../pipeline/datasets/amazon_google/amazon_google-heuristic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the schema matching dataset in the heuristic case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'output_file': 'amazon-google',\n",
    "    'concatenate': 'horizon',\n",
    "    'missing_value': 'nan,ukn,none,unknown,',\n",
    "    'missing_value_strategy': '',\n",
    "    'round_number': 0,\n",
    "    'round_columns': 'price',\n",
    "    'auto_merge': False,\n",
    "    'tokenize_shared': True \n",
    "}\n",
    "\n",
    "df_c = dp.data_preprocessing([df1.drop('id', axis=1), df2.drop('id', axis=1)], parameters)\n",
    "# df_c = df_c.drop('id', axis=1)\n",
    "df_c.to_csv('../pipeline/datasets/amazon_google/amazon_google-heuristic-sm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the ER match file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "File test.csv: 234 matches.\n",
      "train.csv\n",
      "File train.csv: 699 matches.\n",
      "valid.csv\n",
      "File valid.csv: 234 matches.\n",
      "Total matches: 1167\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "tot_m = 0\n",
    "dir_path = '../pipeline/experiments/amazon-google/exp_data/'\n",
    "with open('../pipeline/matches/matches-amazon_google.txt', 'w') as fo:\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file not in [os.path.basename(_) for _ in [f1, f2]]:        \n",
    "            print(file)\n",
    "            m = 0\n",
    "            with open(dir_path + file, 'r') as fp:\n",
    "                for idx, line in enumerate(fp):\n",
    "                    m1, m2, flag = line.rstrip().rsplit(',')\n",
    "                    if flag == '1':\n",
    "                        s = 'idx_{0},idx_{1}\\n'.format(m1, str(int(m2) + len(df1)))\n",
    "                        fo.write(s)\n",
    "                        m+=1\n",
    "            print('File {}: {} matches.'.format(file, m))\n",
    "            tot_m+=m\n",
    "print('Total matches: {}'.format(tot_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0_manufacturer', '0_price', '0_title', '1_manufacturer', '1_price',\n",
       "       '1_title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pipeline/matches/sm_matches-amazon_google.txt'):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare basic config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = '''smoothing_method:no\n",
    "window_size:3\n",
    "n_dimensions:300\n",
    "sentence_length:60\n",
    "walks_strategy:basic\n",
    "ntop:10\n",
    "ncand:1\n",
    "max_rank:3\n",
    "learning_method:skipgram\n",
    "training_algorithm:word2vec\n",
    "n_sentences:default\n",
    "experiment_type:ER\n",
    "task:train-test\n",
    "with_cid:all\n",
    "with_rid:first\n",
    "numeric:no\n",
    "backtrack:True\n",
    "match_file:\n",
    "write_walks:True\n",
    "output_file:\n",
    "input_file:\n",
    "dataset_info:\n",
    "test_dir:\n",
    "flatten:false\n",
    "embeddings_file:\n",
    "intersection:true'''.split('\\n')\n",
    "\n",
    "parameters = {_.split(':')[0]: _.split(':')[1] for _ in pars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['input_file'] = 'pipeline/datasets/amazon_google/{}'.format('amazon_google-heuristic.csv')\n",
    "parameters['match_file'] = 'pipeline/matches/matches-{}'.format('amazon_google.txt')\n",
    "parameters['dataset_info'] = 'pipeline/info/info-{}'.format('amazon_google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pipeline/config_files/amazon_google/amazon_google-ER-noflatten-int', 'w') as fp:\n",
    "    for k,v in parameters.items():\n",
    "        s = '{}:{}\\n'.format(k,v)\n",
    "        fp.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (EmbDI)",
   "language": "python",
   "name": "pycharm-f75c726a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
